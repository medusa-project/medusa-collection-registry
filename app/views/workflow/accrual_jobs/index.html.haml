%h2 Overview
%p MCR = Medusa Collection Registry
%p 'Files' can mean binary objects in an object store for the purpose of this document. See File Organization for additional context.
%p

%h3 Types
%p Files ingested into MCR are initiated by two processes: Browser and Message
%p
  %strong Browser:
  From within the MCR web application interface, digital preservation staff initiate a bulk ingest from files organized in directories on campus-managed filesystem storage using methods detailed in the Medusa Collection Registry Training: https://wiki.illinois.edu/wiki/display/LibraryDigitalPreservation/Medusa+Collection+Registry+Training
  A bulk ingest is initiated by Digital Preservation staff using the steps outlined in:
  %a(href="https://wiki.illinois.edu/wiki/display/LibraryDigitalPreservation/User+Documentation" )
    Medusa Collection Registry Training
  The files that have been identified and prepared for ingest are sometimes referred to as
  %em staged content.
  The source storage is are NSF mounted on the MCR EC2.
%p
  %strong Message:
  Associated web applications such as IDEALS or Illinois Data Bank automatically initiate a one-file-at-a-time ingest of objects staged in an associated S3 bucket. The associated apps send messages providing some system metadata along with the source and destination roots and keys (using the medusa storage gem framework).
%h3 Steps
%ol
  %li
    %p Collection and bit-level file group exist within MCR, and are specified in the initiation process.
  %li
    %p File(s) are copied from source storage to a specific storage key within the namespace associated with the specified bit-level file group.
  %li
    %p CfsDirectory and BitLevelFileGroup objects are created for subdirectories.
  %li
    %p CfsFile objects are created for files.
  %li
    %p Technical metadata is extracted from each file -- e.g., size, checksum, media type.
  %li
    %p Some technical metadata is immediately available, such as size.
  %li
    %p An initial guess of media type is determined from the file extension, if any.
  %li
    %p A checksum is calculated during the copy phase for files up to sized 5 MB in size as calculated by AWS. That is because AWS uses md5 checksum for the eTag value of the object for objects up to that size, and uses another algorithm for larger objects.
  %li
    %p Fits analysis is performed on each file, updating checksum and media type of the CfsFile object.
  %li
    %p Checksum is calculated if it cannot be extracted from the Fits file.
  %li
    %p MCR reports completion to the initiating agent.

%h2 High Level Technical Implementation

%p
  Once a user clicks "Ingest" in step 4 specified in the
  %a(href="https://wiki.illinois.edu/wiki/display/LibraryDigitalPreservation/User+Documentation" )
    Medusa Collection Registry Training
  a Workflow::AccrualJob object is created. All Workflow::AccrualJob objects (accruals) are listed
  in the dashboard as described in the training.

%p
  %strong Delayed Jobs
  Asynchronous background jobs are handled using https://github.com/collectiveidea/delayed_job The agents running the delayed jobs are called daemons. The are part of Ruby on Rails, but are started and stopped using a command distinct from starting and stopping the web application offered in the browser, so one can be running without the other.

%h3 States
%p A Workflow::AccrualJob is in one of the following states:
%p
  %strong Start
  The only thing that happens in the start state is moving onto the checking sync state.
%p
  %strong Checking sync
  The only thing that happens in the checking sync state is moving into the checking for existing files state.

%p
  %strong Checking for existing files
  The directories and files in the staged content are compared to the objects already ingested. If duplicates are found and the overwrites are specified as not allowed in the ingest then a report is sent to the initiator of the ingest and the Workflow::AccrualJob goes to the end state. If overwrites are allowed, the check is still done and the results reported. If no duplicates are found or overwrite is allowed, then Workflow::WorkflowAccrualKey objects are created for all of the staged content files, an approval prompt email is sent, and the Workflow::AccrualJob changes state to awaiting approval.

%p
  %strong Awaiting approval
  A Proceed or Abort option is offered in the dashboard. If Proceed is clicked by an authorized user, a message is sent to the associated administrator and the state is changed to awaiting admin approval. The system does not check if the initiator is also an admin. This is a potential place for an improved efficiency.

%p
  %strong Awaiting admin approval
  A Proceed or Abort option is offered in the dashboard. If Proceed is clicked by an authorized user, the system checks if it is using Globus. Using Globus is a configurable option, because we do not want to use it in development or testing instances, and may want alternatives in the future. Currently both demo and production instances are configured to use Globus. The state is changed to sending copying messages.

%p
  %strong Copying
  This state is only used when not using Globus for copying. Files are copied using different methods for object stores, such as AWS S3 buckets vs filesystems, but generally requires a fast, reliable connection to the staging and medusa storage systems. When copying is finished, the state is changed to starting assessments.

%p
  %strong Sending copying messages
  This state is only used when using Globus for copying. A Workflow::GlobusTransfer is created for each Workflow::AccrualKey and a transfer request call is made to Globus for each individual file. After all transfer requests are sent, the state is changed to await copy messages.

%p
  %strong Awaiting copy messages
  This state is only used when using Globus for copying. Each Workflow::AccrualKey's Workflow::GlobusTransfer status is checked. Checking the status first checks if the object has been copied. If it has been copied, the Workflow::AccrualKey record is destroyed. If there is a reported error, that error is recorded. If there is no error, but the object has not yet been copied, then nothing happens during that check. This happens in a loop until all of the transfers succeed or have a reported error. Currently, errors need to be cleared up manually by developer. More transparency and automation would be useful in this step. After all files are copied, the state changes to starting assessments.

%p
  %strong Starting Assessments
  The cfs_directory (see File Organization) specified in the ingest request is prompted to initiate two parallel asynchronous background tasks: one to examine the storage tree associated with that cfs_directory and make the Medusa records match what exists in storage, and the other to send calls to the Medusa Assessor Service to get results of checksum calculation, media type detection, and fits processing for any files that don't already have those done, which is expected to be exactly the newly ingested objects. Once those tasks have been initiated, the state changes to running assessment.

%p
  %strong Running Assessment
  In a loop, each of the processes initiated in the previous state are checked until they are all complete. More transparency and automation would be useful in this step. After all is complete, the state changes to emailing completion.

%p
  %strong Emailing completion
  The done email is sent and the state changes to ending.

%p
  %strong Aborting
  This state is not used by every Workflow::AccrualJob, only as necessary. A Workflow::AccrualJob in this state sends an aborted message and then the state changes to end.

%p
  %strong Ending
  Destroys the Workflow::AccrualJob record and any associated Delayed::Job record.