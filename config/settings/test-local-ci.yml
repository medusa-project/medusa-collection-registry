#allowed amqp accrual clients
#for each a storage root with the same name must be configured in the storage.amqp section
admin:
  :netids: admin
  :localpass: localpass
amqp_accrual:
  placeholder_md5: generate_SecureRandom_hex
  idb:
    #set to false to turn off this accrual client
    active: true
    #set to true to allow deletion messages to work
    allow_delete: true
    #queue for processing accruals
    delayed_job_queue: idb
    #amqp queues for exchanging messages about accruals with this client
    incoming_queue: idb_to_medusa_test
    outgoing_queue: medusa_to_idb_test
    #file group that will take content from this accrual client
    file_group_id: 1234
    #whether or not to return some extra information in amqp messages - this may be restructured to allow clients to be
    #configured to get exactly the information they want
    return_directory_information: true
  dspace:
    #set to false to turn off this accrual client
    active: true
    #set to true to allow deletion messages to work
    allow_delete: true
    #queue for processing accruals
    delayed_job_queue: dspace
    #amqp queues for exchanging messages about accruals with this client
    incoming_queue: dspace_to_medusa_test
    outgoing_queue: medusa_to_dspace_test
    #file group that will take content from this accrual client
    file_group_id: 5678
    #whether or not to return some extra information in amqp messages - this may be restructured to allow clients to be
    #configured to get exactly the information they want
    return_directory_information: true
assessor:
  cluster: na
  subnets:
    - na
    - na
  security_groups:
    - na
  container_name: na
  platform_version: "0.0.0"
  task_definition: na
aws:
  queue_mode: local
  region: us-east-2
book_tracker:
  items_url: https://book-tracker.example.com/books
  tasks_url: https://book-tracker.example.com/tasks
#If this is set then Workflow::AccrualJobs (and potentially other accruals in the future) may use a
#separate copying server to do copying work. The queues are hooked up with the medusa.amqp information.
#The copying server is used if both the source and target root names appear in the 'roots' config below,
#where these roots are the the names used in the storage section. It is assumed that the copying server
#is configured to translate these root names to the correct storage endpoints.
copy_server:
  incoming_queue: copier_to_medusa_test
  outgoing_queue: medusa_to_copier_test
  roots:
    - storage_root_name_1
    - storage_root_name_2
#The root of the DLS application, needed to make links and such from the collection registry
dls:
  base_url: https://dls.example.com
#this is a map of netids to an array of collection ids - the users will get
#download and export permissions on files in that collection. Intended to allow this in
#a temporary, ad-hoc fashion
download_users: {}
#settings for communication with downloader
downloader:
  #downloader root - configured on the other side as well
  root: medusa
  #queues to use
  incoming_queue: downloader_to_medusa_test
  outgoing_queue: client_to_downloader_test
  #Bunny style amqp connection options - note that these are distinct from the main amqp options because the downloader
  #may be on a different vhost/etc.
  amqp: {}
fits:
  #default size to do
  batch_size: 1000
  #In certain cases it may be advantageous to call a fits binary directly, but this is optional
  binary: tbd
  #url for fits server
  server_url: http://localhost:4567
  incoming_queue: fits_to_medusa_test
  outgoing_queue: medusa_to_fits_test
#medusa_host is used by the models to help generate the appropriate URLs to
medusa_host: medusa.library.illinois.edu
#queue configuration
message_queues:
  assessor_to_medusa_url: http://localhost:9324/queue/assessor-to-medusa
#connection to the iiif image server
iiif:
  host: localhost
  port: 43002
  root: iiif/2
ldap:
  host: ad.uillinois.edu
  port: 389
  protocol: tls
  user: CN=IDEALSLookup,OU=SU Accounts,OU=Library,OU=Urbana,DC=ad,DC=uillinois,DC=edu
  passwd: "@-?Refactoring7@-?"
  base: DC=ad,DC=uillinois,DC=edu
  search: ^CN=([^,]+),((CN|OU)=[^,]+,)*DC=ad,DC=uillinois,DC=edu$
mailer:
  #You can set the following to add an extra bit into the subjects of emails that go out,
  #making is easier to distinguish production/staging/etc. This is done with an interceptor in config/initializers.
  # system_name:
#general medusa configuration
medusa:
  #medusa server information for generating certain urls that need more than just the path
  server: http://localhost:3000
  #'cfs' staging file config. Note that this doesn't strictly need to be a dx/cfs file system,
  #but we use that name in order to distinguish from our internal bit file stuff for now.
  cfs:
    #optional - set if you have a local fits to use
    fits_home: na
  #This opens a few actions to basic auth so that machine clients can use them
  basic_auth: na:na
  #AD groups used to allow use and full administration of medusa
  medusa_users_group: na
  medusa_admins_group: na
  medusa_project_admins_group: na
  medusa_superusers_group: na
  #approximate number of days to elapse between fixity checks
  fixity_interval: 9999
  fixity_server:
    #AMQP queues to communicate with fixity server
    outgoing_queue: medusa_to_fixity_test
    incoming_queue: fixity_to_medusa_test
  #You can set anything here that you might pass to Bunny.new. Anything you don't set just gets the Bunny default.
  #Typical fields needed for our setup are shown below.
  amqp:
    #FOR LOCAL USE
    port: 5672
  #FOR USE WITH TEST SERVER
  #    ssl: true
  #    host: rabbitmq-dev.library.illinois.edu
  #    user: medusa
  #    password: cestUmTi
  #    vhost: medusa-test
  #    verify_peer: false
  public_view_on: false

sqs:
  queue_mode: local


#configuration for Omniauth::Shibboleth
shibboleth:
  host: na
  uid_field: na
  extra_fields:
    - na
  request_type: header
  info_fields:
    email: mail
smtp:
  #These are settings as expected for ActionMailer::Base.smtp_settings
  smtp_settings:
    address: na
    domain: na
  web_host: "localhost:3000"
#These are config for roots or lists of roots as required by the medusa_storage gem.
#You may want to use YAML facilities to extract commonalities for S3 roots if they exist
storage:
  main_root:
    :type: s3
    :endpoint: "http://minio:9000"
    :region: "us-east-2"
    :aws_access_key_id: "minioadmin"
    :aws_secret_access_key: "minioadmin"
    :force_path_style: true
    :bucket: medusa-local-main
  #For some operations if the main bucket is S3, it is useful and much more efficient to have this
  #bucket also mounted via 'rclone mount'. If that is done, configure a filesystem root under
  #the main_root_rclone key. For example, FITS generation and running 'file' are much
  #improved this way, requiring no temporary copy of the S3 object to be created. In fact,
  #this _could_ be used more generally - all that is required is that things that use it only read
  #and that whatever it _is_ exposes the main_root as a filesystem. So in a theoretical sense it
  #would work if that were, say, an AWS storage gateway, an alternate fuse type mount of a bucket, etc.
  main_root_rclone: ~
  #set this if there is a backup location. It is used only with the file_group delete
  #procedure in order to make the final delete apply to the backup copy as well.
  main_root_backup: ~
  amqp:
    - :name: idb
      :type: filesystem
      :path: <%= Rails.root %>/tmp/idb-staging/test
#where project content for automatic ingest is staged
  project_staging:
    :name: project_staging
    :type: filesystem
    :path: tbd
  #location to use for temporary storage - note on S3 EC2 machines you might
  #want to put this on an EFS volume, as their local storage is typically small.
  #Note that we have code to use local storage for 'small' files, whatever that
  #means, that will eventually be configurable
  tmpdir: tbd
  #For each root the name gives a tag to be shown to start in the browser when accruing
  accrual:
    - :name: staging-1
      :type: filesystem
      :path: tbd
  assessor:
    :name: assessor
    :type: filesystem
    :path: tbd
  fits:
    :name: fits
    :type: filesystem
    :path: tbd
  reports:
    :name: reports
    :type: filesystem
    :path: tbd
globus:
  client_id: na
  client_secret: na
  accrual:
    - :name: na
      :uuid: na
      :path: /
alma:
  :host: na
  :key: na
#settings for triple store integration
#triple_store:
#  triple_store_base_url: http://localhost:3030/test
#  medusa_base_url: http://localhost:3000
#  medusa_rdf_prefix: https://medusa.library.illinois.edu/terms/
